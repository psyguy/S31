---
title: "Lab 2B"
subtitle: "Feature selection"
date: "`r format(Sys.time(), '%Y')`"
params:
  answers: true
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
    number_sections: true
    theme: paper
    highlight: tango
    code_folding: hide
    pandoc_args: --output=2B-Lab-FeatureSelection-answers.html
---


```{r message = FALSE}
knitr::opts_chunk$set(eval=params$answers, message=FALSE)
library(ISLR)
library(tidyverse)
library(glmnet)
library(caret)
library(gridExtra)
```


---



For this lab you need the packages `ISLR`, `ggplot2`, `glmnet`, `caret`, and `gridExtra`. 

The data set for this lab is `College`, check the help page for its content. 

The lab is structured as follows:

1. Explore, normalize and standardize the data

2. Split data in train/test set

3. Train filter, wrapper and embedded methods, and compare test MSE

4. Introduce two-way interactions effects, and train the lasso


# Data preparation

a. Display a summary of the data set. Check for impossible values (e.g. percentage outside the range 0-100), and delete these cases from the data (overwrite the original data).

```{r}
summary(College)

College <- College %>% 
  filter(Grad.Rate <= 100,
         PhD <= 100)
```


b. Since we are going fit linear models, we want the variables in the data to have approximately normal distributions. To visually check for skew and outliers, standardize all variables in the data set (first convert the dichotomous factor `Private` into a numeric variable), and display the boxplots of the standardized variables in a single plot array.


```{r}
College %>% 
  mutate(Private = as.numeric(Private),
         across(everything(), scale)) %>% 
  pivot_longer(everything(), values_to = "z-score", names_to = "variable") %>% 
  ggplot(aes(`z-score`, variable)) +
  geom_boxplot() +
  theme_minimal()
```


c. Apply a normalizing log transformation to the variables with heavy skew (for variables with left skew, first reverse the scores). Make sure that before you transform the variables their minimum score is 1, in order to avoid taking the log of 0, which is infinite. Display the boxplots of the transformed variables to check if the transformation had the desired effect, and then save the resulting data (overwrite the previous version of `College`). 

```{r}
College <- College %>% 
  mutate(Private = as.numeric(Private),
         across(c(PhD, Top25perc, Terminal), ~{101 - .}),
         across(c(2, 3, 4, 5, 7, 8, 11, 12, 13, 14, 17), ~ log(. + 1)),
         across(everything(), scale)) 
  
College  %>% 
  pivot_longer(everything(), values_to = "z-score", names_to = "variable") %>% 
  ggplot(aes(`z-score`, variable)) +
  geom_boxplot() +
  theme_minimal()
```

d. Display 16 scatterplots with `Grad.Rate` on the y-axis and the respective 16 numeric predictors on x-axis (so exclude `Private`). Check the scatterplots for non-linearity. 

```{r fig.asp=1}
par(mfrow = c(4, 4))
for(i in 2:17)plot(College[, c(i, 18)], cex = .7)
```



---

# Train/test set

a. Create the vector `train` with the row numbers for the train set containing of 75% of the cases. Set the seed to 1 for reproducibility of the results.

```{r}
set.seed(1)
train <- createDataPartition(y = College$Grad.Rate, p = .75, list = FALSE)
```


---

## Filter method

Filter methods select a set of features before training the model. For these we will select features from the training set based on a minimum correlation with `Grad.Rate`. As selection criterion we use an absolute correlation larger than 0.3.

a. Display the last column of the correlation matrix of the training set (the 18th column pertaining to outcome variable `Grad.Rate`). Which features satisfy the criterion?

```{r}
cor(College[train, ])[-18, 18] %>%  round(3) 
```

b. Make a character vector `vars` containing the names of the features that satisfy our selection criterion.


```{r}
vars <- which(abs(cor(College[train, ])[-18, 18]) > .3)
```

c. Fit the linear model to the training data with the selected features as predictors of `Grad.Rate`, and save the object as `train_filter`.

```{r}
train_filter <- lm(Grad.Rate ~ ., data = College[train, c(vars, 18)])
```


d. Display the summary of `train_filter`. Although all predictors have an absolute correlation of at least .3 with `Grad.rate`, some of them are not significant as predictor? Why is that?

```{r}
summary(train_filter)
```

e. Obtain and save the predictions for the test set

```{r}
filter_pred <- predict(train_filter, newdata = College[-train, ])
```

f. Compute and display the test MSE. Save this value as `filter_mse`.

```{r}
filter_mse  <- mean((College[-train, "Grad.Rate"] - filter_pred)^2)
```

---

## Wrapper method

Wrapper methods select features while training the model. Here we use the backward step-wise selection procedure. Check the help page of the function `step()` for assistance on how to conduct a backward step-wise selection. 

a. Fit the linear model with all features as predictors, and apply the backward step-wise  `step()` procedure. Save the result under the name `train_step`. 

```{r}
train_step <- step(lm(Grad.Rate ~ ., College[train, ]))
```

b. Display the summary of the step model. Compare the explained variance of this model and the filter model. Which model performs best on the training data?

```{r}
summary(train_step)
```


c. Obtain and save the predictions of the step model for the test set.


```{r}
pred_step  <- predict(train_step, newdata = College[-train, ])
```


d. Compute the test MSE for the step model, and save it as `step_mse`.

```{r}
(step_mse <- mean((College[-train, "Grad.Rate"] - pred_step)^2))
```


## Embedded method

In the embedded method, the feature selection process is embedded in the model training phase. We use the function `glmnet()` to fit the  lasso and ridge models to the training data. This function computes the deviance for a sequence of values of the hyperparameter `lambda`, which determines the budget for the coefficients. The function `cv.glmnet()` is then applied to object created with `glmnet()` to determine the optimal value for `lambda` with cross-validation. 

a. Create the objects `train_lasso`  and `train_ridge` with `glmnet()`. Set the argument `alpha = 1` for the lasso, and `alpha = 0` for the ridge. Do not forget that the argument `x` has to be of class `matrix`!

```{r}
train_ridge <- glmnet(x = as.matrix(College[train, -18]), 
                      y = College[train, 18], 
                      alpha = 0)

train_lasso <- glmnet(x = as.matrix(College[train, -18]), 
                      y = College[train, 18], 
                      alpha = 1)
```


b. Plot both `glmnet` objects. Interpret the plots.

```{r fig.asp=1.2}
par(mfrow=c(2,1))
plot(train_ridge, label = T)
plot(train_lasso, label = T)
```


c. Cross-validate with `cv.glmnet()` to find the optimal `lambda` values for `alpha=0` an `alpha=1`. Include the argument `type.measure = "mse"`. Save the objects as `cv_ridge` and `cv_lasso`.

```{r}
cv_ridge <- cv.glmnet(x = as.matrix(College[train, -18]), 
                      y = College[train, 18], 
                      alpha = 0, 
                      type.measure = "mse")

cv_lasso <- cv.glmnet(x = as.matrix(College[train, -18]), 
                      y = College[train, 18], 
                      alpha = 1, 
                      type.measure = "mse")
```


d. Plot both `cv.glmnet`  objects, and interpret.

```{r}
par(mfrow=c(1, 2))
plot(cv_ridge)
plot(cv_lasso)
```

e. Display a matrix with the coefficients of linear model with all features as predictors, and of the objects `cv_ridge` and `cv_lasso` in the columns. Round all coefficients to three decimals. Compare the effects of the shrinkage.

```{r}
round(cbind(coef(lm(Grad.Rate ~ . , College[train, ])), coef(cv_ridge), coef(cv_lasso)), 3)
```


f. Obtain the predictions for the test set  - check the help page for `predict.cv.glmnet()` -  and compute and the test MSE's for the ridge and lasso.
 
```{r}
pred_ridge <- predict(cv_ridge, newx = as.matrix(College[-train, -18]))

pred_lasso <- predict(cv_lasso, newx = as.matrix(College[-train, -18]))


ridge_mse  <- mean((College[-train, "Grad.Rate"] - pred_ridge)^2)

lasso_mse  <- mean((College[-train, "Grad.Rate"] - pred_lasso)^2)
```


g. Compare the test MSE for the filter method, the `step` function, and the ridge and lasso. Which method performs work best on the test data.

```{r}
data.frame(filter_mse, step_mse, ridge_mse, lasso_mse)
```




---


# Interactions

So far we have not considered any pairwise interactions in our models. It may very well be that that the effect of one feature on `Grad.rate` is moderated by another feature. The problem with finding relevant interactions is that there are a lot of them (${17\choose2}=136$). The lasso, however, is particularly suited for such a task.

a. Obtain the model matrix of the model `Grad.Rate ~ .^2` for the training set (excluding the intercept), and cross-validate the lasso version of this model with `cv.glmnet()`. Save the the result as `cv_lasso2`

```{r}
cv_lasso2 <- cv.glmnet(x = model.matrix(Grad.Rate ~ .^2, College)[train, -1], 
                       y = College$Grad.Rate[train], 
                       alpha = 1, 
                       type.measure = "mse")
```

b. Plot the object. How many variables have nonzero coefficients at the optimal value for `lambda`?

```{r}
plot(cv_lasso2)
```


c. Display the coefficients. How many of these are interaction effects?

```{r}
round(coef(cv_lasso2) , 3)
```

d. Compute the test MSE for `cv_lasso2`. How does it compare to the models without interactions?
 
```{r}
pred_lasso2 <- predict(cv_lasso2, newx = model.matrix(Grad.Rate ~ .^2, College)[-train, -1])
(lasso2_mse   <- mean((College[-train, "Grad.Rate"] - pred_lasso2)^2))
```



---

END OF LAB