---
title: "Lab 3A"
subtitle: "Logistic regression and LDA"
date: "`r format(Sys.time(), '%Y')`"
params:
  answers: true
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
    number_sections: false
    theme: paper
    highlight: tango
    code_folding: hide
    pandoc_args: --output=3A-Lab-Logistic-answers.html
---


```{r setup, include=F, message=F}
knitr::opts_chunk$set(eval=params$answers, message=FALSE)

library(MASS)
library(glmnet)
library(caret)
library(ggplot2)
library(dplyr)
library(tidyr)
library(pROC)
```

---



The variable `type` of the data set `kernlab::spam` with 4601 e-mails, of which a part is `spam`. 

In this lab we train models to classify emails as either spam or non-spam based 57 features of the email. We compare the results of logistic regression with or without regularization, and of linear and quadratic discriminant analyses.


# Data exploration

a. Load the package `kernlab` and load the `spam` in workspace with the function `data()`. Check its help file for the description of the data. 

```{r}
library(kernlab)
data("spam")
```

b. Display a summary of the features after standardization (so exclude `type`). Compare the minimum and maximum *z*-scores of the variables. Optionally, also display the boxplots of the standardized features. What does these statistics (and plots) tell you about the distributions of the variables?


```{r}
summary(scale(spam[, -58]))
```

c. To avoid huge outliers, cut the scores of all 57 features into 4 approximately equal parts (TIP: use  `mutate(across(1:57, ntile, 4)`. Save the new data set as `spam4`.


```{r}
spam4 <- spam %>% 
  mutate(across(1:57, ntile, 4))
```


---

# Data partitioning

We first make a vector to partition the data in a training set consisting of 75% of the data, and a test of 25% of the data.

a. Set the seed to 10 for reproducibility, and make the vector `train` consisting of 75% of the row numbers with the function `createDataPartition()`.


```{r}
set.seed(10) 
train <- createDataPartition(y = spam4$type, p = .75, list = FALSE)
```

# Model training


We first train the three logistic regression models (one without and two with regularization), and the two  (linear and quadratic) discriminant models.


## Logistic regression

a. Fit the logistic regression model with all 57 features to the training set, and save the object as `train_glm`.

```{r}
train_glm <- glm(type ~ . , family = "binomial", data = spam4[train, ])
```

The warning  `glm.fit: fitted probabilities numerically 0 or 1 occurred` implies that some probability estimates are on the boundary, and that some features have extreme parameter estimates and standard errors.

b. Display the parameter estimates and with `coef(summary(train_glm))`, rounded to two decimals, and identify the features with extreme parameters estimates and standard errors.  

```{r}
round(coef(summary(train_glm)), 2)
```

## Lasso and ridge regression

To avoid such extreme estimates, we can performs logistic regression with a ridge and lasso shrinkage. The function `glmnet()` does this with the arguments `family = "binomial"` and  `alpha = 1` for the lasso and `alpha = 0` for the ridge. Do not forget that the argument `x` for the predictors requires an object of class `matrix`. 

a. Train the model with both the lasso and the ridge penalty and save the objects as `train_lasso` and `train_ridge`. 


```{r}
train_lasso <- glmnet(x = as.matrix(spam4[train, -58]), y = spam4[train, 58], family = "binomial")
train_ridge <- glmnet(x = as.matrix(spam4[train, -58]), y = spam4[train, 58], family = "binomial", alpha = 0)
```

b. Plot both objects with the arguments `label = TRUE`and `xvar = "lambda"`. Which coefficients are most affected by the shrinkage. 

```{r}
plot(train_lasso, label = TRUE, xvar="lambda")
plot(train_ridge, label = TRUE, xvar="lambda")
```


c. To determine the optimal value for the shrinkage parameter `lambda`, perform a 5-fold cross-validation on the both objects. Save the objects as `train_cv.lasso` and `train_cv.ridge`.  

```{r}
set.seed(1)
train_cv.lasso <- cv.glmnet(x = as.matrix(spam4[train, -58]), 
                            y = spam4[train, 58], 
                            family = "binomial",
                            nfolds = 5)
train_cv.ridge <- cv.glmnet(x = as.matrix(spam4[train, -58]), 
                            y = spam4[train, 58], 
                            family = "binomial",
                            nfolds = 5,
                            alpha = 0)
```


d. Plot the object `train_cv.lasso` and `train_cv.lasso`, and interpret the plots. 

```{r}
plot(train_cv.lasso)
plot(train_cv.ridge)
```

e. Display the parameter estimates of `train_glm` and `spam.cv.lasso` and `spam.cv.ridge` side-by-side with the fucntion `cbind()`, rounded to two decimals.

```{r}
round(
  cbind(
    coef(train_glm), 
    coef(train_cv.lasso), 
    coef(train_cv.ridge)), 
  2)
```

## LDA


a. Perform a linear and quadratic discriminant analysis on the `spam4` data. Save the fitted objects under an appropriate name (e.g. `train_lda` and `train_qda`), and display and interpret their content. 

```{r}
train_lda <- lda(x = spam4[train, -58], grouping = spam4[train, 58])
train_lda
train_qda <- qda(x = spam4[train, -58], grouping = spam4[train, 58])
train_qda
```


b. Plot the fitted LDA object, and interpret its meaning. 

```{r}
plot(train_lda)
```

# Model testing

a. Use the function `predict()` to get the predictions of the three logistic models and the two discriminant analyses on the test set. Use the argument `type = "response"` for the logistic models. Save the objects under an appropriate name (e.g. `pred_glm`, `pred_lasso`, etc.).


```{r}
pred_glm    <- predict(train_glm, 
                            newdata = spam4[-train, -58], 
                             type = "response")

pred_lasso  <- predict(train_cv.lasso, 
                            newx = as.matrix(spam4[-train, -58]), 
                            type = "response") 

pred_ridge  <- predict(train_cv.ridge, 
                            newx = as.matrix(spam4[-train, -58]), 
                            type = "response") 


pred_lda    <- predict(train_lda, spam4[-train, -58])

pred_qda    <- predict(train_qda, spam4[-train, -58])

```


b. Check the content of the predicted objects, and for each model compute or extract a factor with the predicted  classifications "nospam" and "spam". Save these under an appropriate name (e.g. `class_glm`, `class_lasso`, etc.).

```{r}
class_glm   <- factor(pred_glm > .5, 
                         labels = c("nonspam", "spam"))

class_lasso <- factor(pred_lasso > 0.5, 
                             labels = c("nonspam", "spam"))

class_ridge <- factor(pred_ridge > 0.5, 
                             labels = c("nonspam", "spam"))

class_lda   <- pred_lda$class

class_qda   <- pred_qda$class
```


c. Display the confusion matrices of the three logistic and two discriminant models, and save them under an appropriate names. 

```{r}
(conf_glm   <- table(observed  = spam4$type[-train], 
                   predicted = class_glm))

(conf_lasso <- table(observed  = spam4[-train, 58], 
                       predicted = class_lasso))

(conf_ridge <- table(observed  = spam4[-train, 58], 
                       predicted = class_ridge))

(conf_lda   <- table(observed  = spam4[-train, 58], 
                     predicted = class_lda))

(conf_qda   <- table(observed  = spam4[-train, 58], 
                     predicted = class_qda))

```



d. Compute and display the misclassification test error rates of the five models (to compute these, copy-paste and run the function `miscl <- function(x) 1  - sum(diag(x)) / sum(x)`, and then fill in the name of the model's confusion matrix at `x`).

```{r}
miscl <- function(x) 1 - sum(diag(x)) / sum(x)
```


```{r}
data.frame(glm   = miscl(conf_glm),
           lasso = miscl(conf_lasso),
           ridge = miscl(conf_ridge),
           lda   = miscl(conf_lda),
           qda   = miscl(conf_qda)) %>% 
  round(4) 
```

e. In one figure, display the ROC curves (in different colors) for the test set with the predicted probabilities i) generated by the random variable `runif(1150)`, ii) of the lasso model and iii) of the QDA. Interpret the plots.

```{r}
plot(roc(spam4[-train, 58], runif(1150)))
plot(roc(spam4[-train, 58], pred_glm), add = T, col="blue")
plot(roc(spam4[-train, 58], pred_qda$posterior[, 2]), add = T, col="red")
```

