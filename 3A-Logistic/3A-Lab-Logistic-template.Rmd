---
title: "Lab 3A"
subtitle: "Logistic regression and LDA"
date: "`r format(Sys.time(), '%Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
    number_sections: false
    theme: paper
    highlight: tango
---


```{r setup, include=F, message=F}
knitr::opts_chunk$set(message=FALSE)

library(MASS)
library(glmnet)
library(caret)
library(ggplot2)
library(dplyr)
library(tidyr)
library(pROC)
```

---



The variable `type` of the data set `kernlab::spam` with 4601 e-mails, of which a part is `spam`. 

In this lab we train models to classify emails as either spam or non-spam based 57 features of the email. We compare the results of logistic regression with or without regularization, and of linear and quadratic discriminant analyses.


# Data exploration

a. Load the package `kernlab` and load the `spam` in workspace with the function `data()`. Check its help file for the description of the data. 

```{r}

```

b. Display a summary of the features after standardization (so exclude `type`). Compare the minimum and maximum *z*-scores of the variables. Optionally, also display the boxplots of the standardized features. What does these statistics (and plots) tell you about the distributions of the variables?


```{r}

```

c. To avoid huge outliers, cut the scores of all 57 features into 4 approximately equal parts (TIP: use  `mutate(across(1:57, ntile, 4)`. Save the new data set as `spam4`.


```{r}

```


---

# Data partitioning

We first make a vector to partition the data in a training set consisting of 75% of the data, and a test of 25% of the data.

a. Set the seed to 10 for reproducibility, and make the vector `train` consisting of 75% of the row numbers with the function `createDataPartition()`.


```{r}

```

# Model training


We first train the three logistic regression models (one without and two with regularization), and the two  (linear and quadratic) discriminant models.


## Logistic regression

a. Fit the logistic regression model with all 57 features to the training set, and save the object as `train_glm`.

```{r}

```

The warning  `glm.fit: fitted probabilities numerically 0 or 1 occurred` implies that some probability estimates are on the boundary, and that some features have extreme parameter estimates and standard errors.

b. Display the parameter estimates and with `coef(summary(train_glm))`, rounded to two decimals, and identify the features with extreme parameters estimates and standard errors.  

```{r}

```

## Lasso and ridge regression

To avoid such extreme estimates, we can performs logistic regression with a ridge and lasso shrinkage. The function `glmnet()` does this with the arguments `family = "binomial"` and  `alpha = 1` for the lasso and `alpha = 0` for the ridge. Do not forget that the argument `x` for the predictors requires an object of class `matrix`. 

a. Train the model with both the lasso and the ridge penalty and save the objects as `train_lasso` and `train_ridge`. 


```{r}

```

b. Plot both objects with the arguments `label = TRUE`and `xvar = "lambda"`. Which coefficients are most affected by the shrinkage. 

```{r}

```


c. To determine the optimal value for the shrinkage parameter `lambda`, perform a 5-fold cross-validation on the both objects. Save the objects as `train_cv.lasso` and `train_cv.ridge`.  

```{r}

```


d. Plot the object `train_cv.lasso` and `train_cv.lasso`, and interpret the plots. 

```{r}

```

e. Display the parameter estimates of `train_glm` and `spam.cv.lasso` and `spam.cv.ridge` side-by-side with the fucntion `cbind()`, rounded to two decimals.

```{r}

```

## LDA


a. Perform a linear and quadratic discriminant analysis on the `spam4` data. Save the fitted objects under an appropriate name (e.g. `train_lda` and `train_qda`), and display and interpret their content. 

```{r}

```


b. Plot the fitted LDA object, and interpret its meaning. 

```{r}

```

# Model testing

a. Use the function `predict()` to get the predictions of the three logistic models and the two discriminant analyses on the test set. Use the argument `type = "response"` for the logistic models. Save the objects under an appropriate name (e.g. `pred_glm`, `pred_lasso`, etc.).


```{r}

```


b. Check the content of the predicted objects, and for each model compute or extract a factor with the predicted  classifications "nospam" and "spam". Save these under an appropriate name (e.g. `class_glm`, `class_lasso`, etc.).

```{r}

```


c. Display the confusion matrices of the three logistic and two discriminant models, and save them under an appropriate names. 

```{r}

```



d. Compute and display the misclassification test error rates of the five models (to compute these, copy-paste and run the function `miscl <- function(x) 1  - sum(diag(x)) / sum(x)`, and then fill in the name of the model's confusion matrix at `x`).

```{r}

```


```{r}

```

e. In one figure, display the ROC curves (in different colors) for the test set with the predicted probabilities i) generated by the random variable `runif(1150)`, ii) of the lasso model and iii) of the QDA. Interpret the plots.

```{r}

```

