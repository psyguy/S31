---
title: "Lab 3B"
subtitle: "Tree-based methods and SVM's"
date: "`r format(Sys.time(), '%Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: false
    theme: paper
    highlight: tango
---


```{r setup, include=F, message=F}
knitr::opts_chunk$set(message=FALSE)
library(kernlab)
library(tree)
library(randomForest)
library(fastAdaboost)
library(caret)
library(e1071)
library(dplyr)
```

---

In this lab we classify the `spam` data again, but using different techniques than in the previous lab. For this lab you need the packages `kernlab`, `tree`, `randomForest`, `fastAdaboost`, `e1071` and `caret`. 


# Training the models

To compare our results, we use the same training and test set. However, we do not bother to transform the features before we start. Why is this not necessary? 

a. Make the same vector with indices for the training set as in the previous lab. 

```{r}

```

## Tree

a. Grow a tree on the training set. Save the object as `train_tree`, and plot it. 

```{r}

```

b. The fitted may be overfitting. Before we can prune the tree we need to determine the optimal number of knots. Find out this number by performing cross-validation with the function `cv.tree()`.

```{r}

```


d. Prune the tree using the optimal number of nodes from the cross-validation, and save the pruned tree object under an appropriate name.

```{r}

```

## Bagging/random forest

Both models can be trained with the function `randomForest()`. The only argument that needs to be changed is `mtry` (see the function's help page). To learn about the importance of the features, set the argument `importance = TRUE`. 

a. Train the bagging model, save its object as `train_bag`, and display and interpret its content.

```{r}

```

b. Train the random forest model, save its object as `train_rf`, display its content and compare it to that of the random forest object.

```{r}

```


c. Display the variable importance plots of both objects for the 10 most important predictors, and interpret.

```{r}

```


## Boosting

For boosting we use the function `adaboost()` (check out the help page for this function). The function has the hyperparameter `nIter`, which is the number of weak learners that will be used. Remember that too few learners might lead to underfitting, and too many to overfitting. So, we will have to determine the optimal value of `nIter` with cross-validation. If we use the function `train()` for this, we need to specify, aside  `formula` and `data`, the arguments `method = adaboost`,  `tuneGrid = "Adaboots.M1", nIter =  c(10, 25, 50))` and `trControl = trainControl(method = "cv", number = 5)`. Of course, you may also use different and/or more values for `nIter`. However, be aware that such changes may affect the computation time. 

 
a. Apply the function `adaboost()` to the training set. Determine the value of `nIter` yourself (try out different values). Obtain the predictions for the test set, and display the misclassification error rate.

b. Train the boosting model on the training set, and save the object under an appropriate name. 


```{r ada}

```

c. Display and interpret the fitted boosting object.

```{r}

```

## Support Vector Machines 

For training the support vector machine we the function `svm` of the package `e1071` (check its help page). Let's train an extremely flexible one with a radial kernel. This kernel has a hyperparameter `cost` for which the optimal value has to be determined with cross-validation. This is done with the function  `tune(method = svm, data = spam[train, ], kernel = "radial", ranges = list(cost = <sequence numbers>)`. The cost will be somewhere between 0 and 4. 

a. Tune an SVM with radial kernel by cross-validating `cost`. Specify an appropriate sequence of values for `cost` to be cross-validated (warning: do not make the sequence too long, because the cross-validation takes quite some time). Save the object, and display and interpret its summary. 

```{r radial}

```


```{r}

```



# Testing the models

As in the previous labs, we now evaluate the models by looking at misclassification test error. 



a. Use the function `predict()` to obtain the class predictions on the test set for the pruned tree and the bagging, random forest, boosting and svm models.

```{r}

```


```{r}

```

```{r}

```


